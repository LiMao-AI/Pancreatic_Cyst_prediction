{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "from sklearn import linear_model,svm,tree,ensemble,naive_bayes,neighbors,discriminant_analysis\n",
    "from sklearn import model_selection, preprocessing, metrics, feature_selection\n",
    "\n",
    "# for unbalanced data\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import ADASYN,BorderlineSMOTE,KMeansSMOTE,RandomOverSampler,SMOTE,SMOTENC,SVMSMOTE\n",
    "\n",
    "\n",
    "from scipy import stats as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import mifs\n",
    "from copy import copy\n",
    "\n",
    "from utils import get_model, get_selected_features\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training cohort and the testing cohort\n",
    "train_data = pd.read_excel('Path to the training data')\n",
    "test_data = pd.read_excel('Path to the internal testing data')\n",
    "ext_data = pd.read_excel('Path to the external testing data')\n",
    "test_data_list = [test_data,ext_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of data\n",
    "print(train_data['label'].value_counts())\n",
    "print(test_data['label'].value_counts())\n",
    "print(ext_data['label'].value_counts())\n",
    "\n",
    "# As the positive and negative samples in this study exhibit a relatively balanced distribution, \n",
    "# resampling of positive and negative samples is not utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure all feature names\n",
    "feature_names = ['gender', 'age', 'jaundice', 'pancreatitis_history', 'CEA_level', 'CEA_elevating', 'CA199_level', 'CA199_elevating'] +\\\n",
    "                ['tumor_location', 'tumor_size', 'MPD_communication', 'diameter_MPD', \n",
    "                 'dilation_MPD', 'thick_cystic_wall', 'enhanced_cystic_wall', 'septations', 'thick_septations', \n",
    "                 'enhanced_septations', 'mural_nodule_solid_component', 'peripheral_calcification','central_calcification'] \n",
    "\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of total features in feature_names list\n",
    "featurn_num = len(feature_names) \n",
    "\n",
    "# # According to the TRIPOD guideline, the 10 EPV rule was used for the feature num choice \n",
    "select_max_num = min(sum(train_data['label']==0)//10, sum(train_data['label']==1)//10)\n",
    "\n",
    "# Limit the maximum number of features to be selected to the number of total features or select_max_num, whichever is smaller.\n",
    "select_max_num = min(featurn_num, select_max_num)\n",
    "\n",
    "# Create a list of integers ranging from 1 to select_max_num as the number of features to consider\n",
    "n_features = list(range(1, select_max_num+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing ... Delete irrelevant columns from the dataframe\n",
    "train_X = train_data[feature_names].copy()\n",
    "train_y = train_data['label'].copy()\n",
    "\n",
    "test_Xs = []\n",
    "test_ys = []\n",
    "for each_test in test_data_list:\n",
    "    test_Xs.append(each_test[feature_names].copy()) \n",
    "    test_ys.append(each_test['label'].copy())\n",
    "\n",
    "print(train_X.shape)\n",
    "for each_test in test_Xs:\n",
    "    print(each_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the StandardScaler from the preprocessing module\n",
    "mms = preprocessing.StandardScaler()\n",
    "\n",
    "# Select categorical and numerical features separately from feature_names list\n",
    "cat_features = [f for f in feature_names if len(train_X[f].unique())<3]\n",
    "num_features = [f for f in feature_names if f not in cat_features]\n",
    "\n",
    "# Fit and transform the numerical features of train_X using StandardScaler, store as DataFrame and concatenate with categorical features\n",
    "train_X_num = pd.DataFrame(mms.fit_transform(train_X[num_features]), columns=num_features)\n",
    "train_X = pd.concat([train_X[cat_features].reset_index(drop=True), train_X_num], axis=1)\n",
    "\n",
    "# Transform the numerical features of each test set using the same StandardScaler instance used for train_X\n",
    "for i in range(len(test_Xs)):\n",
    "    test_Xs_num = pd.DataFrame(mms.transform(test_Xs[i][num_features]), columns=num_features)\n",
    "    test_Xs[i] = pd.concat([test_Xs[i][cat_features].reset_index(drop=True), test_Xs_num], axis=1)\n",
    "    \n",
    "# Print the shape of train_X and the shape of each test set after transformation\n",
    "print(train_X.shape)\n",
    "for each_test in test_Xs:\n",
    "    print(each_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model_include': ['Logistic Regression','SVM','Decision Tree','Bernoulli NB','Gaussian NB','K Nearest Neighbors',\n",
    "                   'Linear Discriminant Analysis'],\n",
    "    'feature_selection_include':['JMIM','MRMR','JMI','F_Test','L1_based'],\n",
    "    \n",
    "    'save_dir':'Path to the output folder',\n",
    "    \n",
    "    'feature_selection':{\n",
    "        'n_features':n_features,\n",
    "    },\n",
    "    b\n",
    "    'model_params': {\n",
    "        'Logistic Regression':{\n",
    "            'penalty':['l1', 'l2', 'elasticnet', 'none'],\n",
    "            'dual':[True,False],\n",
    "            'C':[0.1, 0.5, 1, 5, 10],\n",
    "            \"fit_intercept\": [True, False],\n",
    "            \"class_weight\": [\"balanced\", None], \n",
    "            'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "\n",
    "        },\n",
    "        'SVM':{\n",
    "            'C':[0.1, 0.5, 1],\n",
    "            'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'degree':[2,3,4],\n",
    "            'gamma':['scale','auto'],\n",
    "            'class_weight':['balanced',None],\n",
    "\n",
    "        },\n",
    "        'Decision Tree':{\n",
    "            'criterion':['gini', 'entropy'],\n",
    "            'max_depth':[2,3,4,None],\n",
    "            'class_weight':['balanced',None],\n",
    "        },\n",
    "        'Random Forest':{\n",
    "            'n_estimators':[10, 50, 100],\n",
    "            'criterion':['gini', 'entropy'],\n",
    "            'max_depth':[2,3,4,None],\n",
    "            'class_weight':['balanced','balanced_subsample',None],\n",
    "            'bootstrap':[True,False]\n",
    "        },\n",
    "        'Ada Boost':{\n",
    "            'n_estimators':[10, 50, 100],\n",
    "            'learning_rate':[1e-4, 1e-3, 1e-2, 0.1],\n",
    "            'algorithm':['SAMME','SAMME.R']\n",
    "                             \n",
    "        },\n",
    "        'Gradient Boosting':{\n",
    "            'loss':['deviance', 'exponential'],\n",
    "            'learning_rate':[1e-4, 1e-3, 1e-2, 0.1],\n",
    "            'n_estimators':[10, 50, 100],\n",
    "            'criterion':['friedman_mse', 'mse', 'mae'],\n",
    "            'max_depth':[2,3,4,None],\n",
    "        },\n",
    "        'XG Boost':{\n",
    "             \"booster\": [\"gbtree\", \"dart\", \"gblinear\"], \n",
    "        },\n",
    "        \n",
    "        'Bernoulli NB':{\n",
    "            'alpha':[1e-4, 1e-3, 1e-2, 0.1, 0.5, 1, 5, 10],\n",
    "            \"fit_prior\": [True, False],\n",
    "        },\n",
    "        'Gaussian NB': {\n",
    "        },\n",
    "        'K Nearest Neighbors':{\n",
    "            \"n_neighbors\": [2,3,4,5],\n",
    "            \"weights\": [\"uniform\", \"distance\"],\n",
    "            \"p\": [1, 2],\n",
    "        },\n",
    "        \"Linear Discriminant Analysis\": {\n",
    "            \"solver\": [\"svd\", \"lsqr\"],\n",
    "        },\n",
    "        \n",
    "    },\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model calculate metrics\n",
    "def calculate(score, label, th):\n",
    "    score = np.array(score)\n",
    "    label = np.array(label)\n",
    "    pred = np.zeros_like(label)\n",
    "    pred[score>=th]=1\n",
    "    pred[score<th]=0\n",
    "    \n",
    "    TP = len(pred[(pred>0.5) & (label>0.5)])\n",
    "    FN = len(pred[(pred<0.5) & (label>0.5)])\n",
    "    TN = len(pred[(pred<0.5) & (label<0.5)])\n",
    "    FP = len(pred[(pred>0.5) & (label<0.5)])\n",
    "    \n",
    "    AUC = metrics.roc_auc_score(label, score)\n",
    "    ACC = metrics.accuracy_score(label, pred)\n",
    "    F1 = metrics.f1_score(label,pred)\n",
    "    AP = metrics.average_precision_score(label, score)\n",
    "    result = {'AUC':AUC, 'acc': ACC, 'F1': F1, \n",
    "              'sen':(TP)/(TP+FN+0.0001),'spe':(TN)/(TN+FP+0.0001),\n",
    "             'AP':AP}\n",
    "#     print('acc',(TP+TN),(TP+TN+FP+FN),'spe',(TN),(TN+FP),'sen',(TP),(TP+FN))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validating \n",
    "\n",
    "The model reached the performance (AUC score) in the cross-validation procedure was selected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take long time...please remain patience \n",
    "\n",
    "# Initialize an empty list to store the results for each feature selection and model combination.\n",
    "rst_dfs = []\n",
    "\n",
    "# Loop over each number of features specified in the config file.\n",
    "for feat_num in config['feature_selection']['n_features']:\n",
    "    \n",
    "    # Loop over each feature selector specified in the config file.\n",
    "    for feat_selector in config['feature_selection_include']:\n",
    "        try:\n",
    "            # Get a list of selected features using the current feature selector, number of features, \n",
    "            # and training data. Subset the training data to include only the selected features.\n",
    "            selected = get_selected_features(feat_selector, feature_names, feat_num, train_X, train_y)\n",
    "            train_data_X = train_X[selected]\n",
    "            \n",
    "            # Print the number of features and feature selector being used.\n",
    "            print('select...... %s ..... features  by %s' % (feat_num, feat_selector))\n",
    "            print(selected)\n",
    "            \n",
    "            # Loop over each model specified in the config file.\n",
    "            for model_name in config['model_include']:\n",
    "\n",
    "                # Initialize an ADASYN sampler and obtain the model and its associated parameters using the \n",
    "                # current model name and config file. Fit the model to the training data with cross-validation \n",
    "                # using grid search to determine the best hyperparameters.\n",
    "                sampler = ADASYN()\n",
    "                model, params = get_model(model_name, config, False, sampler) # not include sampler\n",
    "                rskf = model_selection.RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=0)\n",
    "                grid = model_selection.GridSearchCV(model, params, scoring='roc_auc',cv = rskf, refit=True)\n",
    "                grid.fit(train_data_X, train_y)\n",
    "                \n",
    "                # Obtain the best estimator from the grid search and perform cross-validation on the training data. \n",
    "                # Use the indices obtained from the cross-validation to obtain the predictions for each sample\n",
    "                # in the training data.\n",
    "                fitted_model = grid.best_estimator_\n",
    "                cv_results  = model_selection.cross_validate(estimator=fitted_model, X = train_data_X, y = train_y,\n",
    "                                                            cv = rskf, n_jobs=5, return_train_score=True, \n",
    "                                                             return_estimator=True, verbose=False)\n",
    "                idxes_0 = []\n",
    "                idxes_1 = []\n",
    "                for idx_0, idx_1 in rskf.split(train_data_X,train_y):\n",
    "                    idxes_0.append(idx_0)\n",
    "                    idxes_1.append(idx_1)   \n",
    "                rst_dict = {}\n",
    "                for i in range(len(idxes_1)):\n",
    "                    train_data_cv = train_data_X.iloc[idxes_1[i]]\n",
    "                    model = cv_results['estimator'][i]\n",
    "                    rst = model.predict_proba(train_data_cv)[:,1]\n",
    "                    for idx, idy in enumerate(idxes_1[i]):\n",
    "                        if idy not in rst_dict:\n",
    "                            rst_dict[idy] = []\n",
    "                        rst_dict[idy].append(rst[idx])\n",
    "\n",
    "                # Compute the average validation score and obtain the predictions for each test set using the \n",
    "                # selected features and best estimator obtained from grid search. Save the results to disk \n",
    "                # in CSV format.\n",
    "                val_score = np.mean(pd.DataFrame(rst_dict)[list(range(len(train_data_X)))])\n",
    "                train_score = fitted_model.predict_proba(train_data_X)[:,1]\n",
    "                test_scores = []\n",
    "                for test_X in test_Xs:\n",
    "                    test_X = test_X[selected]\n",
    "                    test_score = fitted_model.predict_proba(test_X)[:,1]\n",
    "                    test_scores.append(test_score)\n",
    "\n",
    "                out_put_dir = os.path.join(config['save_dir'], model_name+'_%s_feat_%s' % (feat_num, feat_selector))\n",
    "                os.makedirs(out_put_dir, exist_ok=True)\n",
    "\n",
    "                # Save results for training, validation, and test sets as csv files\n",
    "                train_rst = pd.DataFrame({'score':train_score, 'label':train_y})\n",
    "                val_rst = pd.DataFrame({'score':val_score, 'label':train_y})\n",
    "                for i, test_score in enumerate(test_scores):\n",
    "                    test_rst = pd.DataFrame({'score':test_score, 'label':test_ys[i]})\n",
    "                    test_rst.to_csv(os.path.join(out_put_dir, 'test_rst_%s.csv' % i), index=False)\n",
    "\n",
    "                train_rst.to_csv(os.path.join(out_put_dir, 'train_rst.csv'), index=False)\n",
    "                val_rst.to_csv(os.path.join(out_put_dir, 'val_rst.csv'), index=False)\n",
    "\n",
    "                # Save the fitted model to a file\n",
    "                pickle.dump(fitted_model, open(os.path.join(out_put_dir, 'model.pkl'),'wb'))\n",
    "\n",
    "                # Calculate the performance metrics for the training, validation, and test sets\n",
    "                fpr_micro, tpr_micro, th = metrics.roc_curve(train_y, val_score)\n",
    "                max_th = -1\n",
    "                max_yd = -1\n",
    "                for i in range(lben(th)):\n",
    "                    yd = tpr_micro[i]-fpr_micro[i]\n",
    "                    if yd > max_yd:\n",
    "                        max_yd = yd\n",
    "                        max_th = th[i]\n",
    "\n",
    "                rst_train = pd.DataFrame([calculate(train_score, train_y, max_th)])[['AUC','acc','F1','sen','spe','AP']]\n",
    "                rst_val = pd.DataFrame([calculate(val_score, train_y, max_th)])[['AUC','acc','F1','sen','spe','AP']]\n",
    "\n",
    "                rst_tests = []\n",
    "                for i, test_score in enumerate(test_scores):\n",
    "                    rst_test = pd.DataFrame([calculate(test_score, test_ys[i], max_th)])[['AUC','acc','F1','sen','spe','AP']]\n",
    "                    rst_tests.append(rst_test)\n",
    "\n",
    "                # Combine the performance metrics for the training, validation, and test sets into a single dataframe\n",
    "                all_rst = [rst_train, rst_val]\n",
    "                all_rst.extend(rst_tests)\n",
    "                rst = pd.concat(all_rst, axis=1)  \n",
    "                rst['model'] = model_name\n",
    "                rst['feat_num'] = feat_num\n",
    "                rst['feature_selector'] = feat_selector\n",
    "                rst_dfs.append(rst)\n",
    "            # Save the selected features to a file\n",
    "            pd.DataFrame({'feat':selected}).to_csv(os.path.join(config['save_dir'], 'feat_%s_%s.csv' % (feat_num, feat_selector)))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "# Combine the results for all feature selection settings and models into a single dataframe\n",
    "rst_df = pd.concat(rst_dfs)\n",
    "\n",
    "# Rename the columns of the dataframe\n",
    "columns = ['AUC_train', 'acc_train', 'F1_train', 'sen_train', 'spe_train', 'AP_train',\n",
    "           'AUC_val', 'acc_val', 'F1_val', 'sen_val', 'spe_val', 'AP_val']+\\\n",
    "            ['%s_test_%s' % (f,v) for v in range(len(test_Xs)) for f in ['AUC','acc','F1','sen','spe','AP'] ]+\\\n",
    "    ['model','feat_num','feature_selector']\n",
    "rst_df = pd.DataFrame(rst_df.values, columns=columns)\n",
    "# Save the dataframe to a file\n",
    "rst_df.to_csv(os.path.join(config['save_dir'], 'model_pf.csv'))\n",
    "\n",
    "# Get the best performing model for each model type based on the validation set AUC\n",
    "best_df = rst_df.groupby('model').apply(lambda x:x.sort_values(by='AUC_val', ascending=False).iloc[0]).reset_index(drop=True)\n",
    "best_df.sort_values(by='AUC_val', ascending=False, inplace=True)\n",
    "best_df.to_csv(os.path.join(config['save_dir'], '_best_models_%s.csv' % config['save_dir'].split('/')[-1]))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
